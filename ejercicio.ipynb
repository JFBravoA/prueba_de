{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b2d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6893e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el url de la web de conagua y un header con una identificación del navegador para evitar problemas de acceso\n",
    "url = 'https://smn.conagua.gob.mx/webservices/?method=1'\n",
    "header = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 \\\n",
    "          (KHTML, like Gecko) Mobile/15E148'}\n",
    "\n",
    "# Se definen las rutas locales de donde se consumirán y se guardarán las tablas del ejercicio:\n",
    "\n",
    "# En esta ruta se consume la data para cruzar\n",
    "ruta_input = 'C:/Users/Usuario/Documents/data_municipios/'\n",
    "\n",
    "# En esta ruta se guardan las tablas con los promedios por municipio \n",
    "ruta_tablas = 'C:/Users/Usuario/Documents/prueba_de/outputs/tablas/'\n",
    "\n",
    "# En esta ruta se guarda la data de los promedios por municipio cruzada con la data más reciente\n",
    "ruta_cruces = 'C:/Users/Usuario/Documents/prueba_de/outputs/tablas_cruzadas/'\n",
    "\n",
    "# En esta ruta se guarda la copia con la data más reciente\n",
    "ruta_current = 'C:/Users/Usuario/Documents/prueba_de/outputs/current/'\n",
    "\n",
    "# En esta ruta se guarda el archivo con los logs\n",
    "ruta_logs = 'C:/Users/Usuario/Documents/prueba_de/outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ec8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos la ruta de guardado, el formato del log y con el modo de guardado en concatenación\n",
    "logging.basicConfig(filename = f'{ruta_logs}logs.log',format = '%(asctime)s %(message)s',filemode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f87606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(url_1, header_1):\n",
    "    \n",
    "    ''' \n",
    "    Descarga datos en formato json comprimidos en gzip desde un servicio web, los procesa y los \n",
    "    retorna como un dataframe de Pandas.\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    url_1: str\n",
    "        URL del servicio web desde el cual se extraerá la data.   \n",
    "    header_1: dict\n",
    "        Encabezado HTTP de la solicitud. Puede incluir información como User-Agent, Content-Type, Date.\n",
    "        \n",
    "    Retorna\n",
    "    -------\n",
    "    data: dataframe\n",
    "        Datos del servicio web estructurados.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    logger_scraping = logging.getLogger()\n",
    "    \n",
    "    try:\n",
    "        result = requests.get(url_1, headers = header_1)\n",
    "        data = gzip.decompress(result.content)\n",
    "        data = json.loads(data)\n",
    "        data = pd.DataFrame(data)\n",
    "        data = data[['ides','idmun','prec','tmax','tmin']]\n",
    "\n",
    "        data['ides'] = data['ides'].astype(int)\n",
    "        data['idmun'] = data['idmun'].astype(int)\n",
    "        data['prec'] = data['prec'].astype(float)\n",
    "        data['tmax'] = data['tmax'].astype(float)\n",
    "        data['tmin'] = data['tmin'].astype(float)\n",
    "        \n",
    "        logger_scraping.setLevel(logging.INFO)\n",
    "        logger_scraping.info('Data extraída con éxito')\n",
    "\n",
    "        return data\n",
    "    \n",
    "    except:\n",
    "        logger_scraping.setLevel(logging.CRITICAL)\n",
    "        logger_scraping.critical('Hubo un error al extraer o descomprimir la data')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac4486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de veces que se repetirá el proceso.\n",
    "n = 100\n",
    "\n",
    "# Contadores\n",
    "i = 0\n",
    "count = 1\n",
    "\n",
    "# Iteración que repite el proceso de extracción y manipulación periódicamente\n",
    "while i <= n:\n",
    "    \n",
    "    # Se usa la función definida anteriomente para obtener la data desde el servicio web\n",
    "    data = getData(url, header)\n",
    "    data2 = pd.DataFrame()\n",
    "    \n",
    "    # Cada que se ejecuta el proceso, la data se concatena en un data frame. Cada segunda hora, los datos se agrupan\n",
    "    # por municipio y se calculan los promedios. Posteriormente, la data se cruza y se guarda en una\n",
    "    # carpeta local.\n",
    "    \n",
    "    # Si el módulo 2 del contador es 0 (es decir, cada que se cumplen 2 horas), se entra en esta condición\n",
    "    if count % 2 == 0:\n",
    "\n",
    "        data2 = pd.concat([data, data2], ignore_index = True)\n",
    "        \n",
    "        # Se agrupan los datos por municipio y se calculan promedios de precipitación y temperatura\n",
    "        data_final = data2.groupby(['ides', 'idmun'], as_index = False).mean()\n",
    "        data_final.rename(columns = {'ides':'Cve_Ent', 'idmun':'Cve_Mun', 'prec':'prec_avg',\n",
    "                                    'tmax':'tmax_avg', 'tmin':'tmin_avg'}, inplace = True)\n",
    "        data_final['tavg'] = data_final[['tmax_avg','tmin_avg']].mean(axis = 1)\n",
    "        \n",
    "        # Se determina la hora y la fecha actual\n",
    "        now = datetime.now()\n",
    "        now = now.strftime(\"%Y%m%d_%H%M\")\n",
    "        \n",
    "        # Se guardan los datos agrupados versionados por fecha y hora\n",
    "        data_final.to_csv(f'{ruta_tablas}{now}.csv')\n",
    "    \n",
    "        logger_data_avg = logging.getLogger()\n",
    "        logger_data_avg.setLevel(logging.INFO)\n",
    "        logger_data_avg.info('Data con promedios creada con éxito')\n",
    "        \n",
    "        # Se determina en automático cuál es la tabla más reciente en la carpeta rovista para realizar el \n",
    "        # cruce con los datos de temperatura y precipitación\n",
    "        lista = os.listdir(ruta_input)\n",
    "        lista_fechas = []\n",
    "\n",
    "        for archivo in lista:\n",
    "\n",
    "            anio = archivo[:4]\n",
    "            mes = archivo[4:6]\n",
    "            dia = archivo[6:8]\n",
    "\n",
    "            date_archivo = pd.to_datetime(f'{anio}-{mes}-{dia}')\n",
    "            date_archivo = date_archivo.date()\n",
    "\n",
    "            lista_fechas.append(date_archivo)\n",
    "\n",
    "        reciente = max(lista_fechas)\n",
    "\n",
    "        id_reciente = int(str(reciente).split('-')[0] + str(reciente).split('-')[1] + str(reciente).split('-')[2])\n",
    "        \n",
    "        # Se lee la tabla más reciente y se realiza el cruce\n",
    "        data_municipios = pd.read_csv(f'{ruta_input}/{id_reciente}/data.csv')\n",
    "        data_municipios = data_municipios.merge(data_final, how = 'left')   \n",
    "        data_municipios.fillna(0, inplace = True)\n",
    "\n",
    "        # Se guardan versiones por fecha y hora en una carpeta y se crea la copia\"current\", que se actualiza cada vez\n",
    "        # que se ejecuta esta condición\n",
    "        \n",
    "        data_municipios.to_csv(f'{ruta_cruces}/{now}.csv', index = False)\n",
    "        data_municipios.to_csv(f'{ruta_current}/current.csv', index = False)\n",
    "        \n",
    "        logger_data_cruce = logging.getLogger()\n",
    "        logger_data_cruce.setLevel(logging.INFO)\n",
    "        logger_data_cruce.info('Data cruzada con éxito')\n",
    "\n",
    "        del data, data2, data_final, data_municipios\n",
    "        \n",
    "        # Se detiene el proceso durante una hora\n",
    "        time.sleep(3600)\n",
    "        \n",
    "    # Si el módulo 2 del contador es diferente de cero (es decir, cada primer hora), los datos extraídos simplemente\n",
    "    # se concatenarán en un dataframe\n",
    "    else: \n",
    "        data2 = pd.concat([data, data2], ignore_index = True)\n",
    "        \n",
    "        # Se detiene el roceso durante una hora\n",
    "        time.sleep(3600)\n",
    "\n",
    "    i += 1\n",
    "    count += 1    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
